{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    import numpy as np\n",
    "    import re\n",
    "    import itertools\n",
    "    from collections import Counter\n",
    "    \n",
    "    \n",
    "    def clean_str(string):\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for datasets.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        return string.strip().lower()\n",
    "    \n",
    "    \n",
    "    def load_data_and_labels():\n",
    "        \"\"\"\n",
    "        Loads polarity data from files, splits the data into words and generates labels.\n",
    "        Returns split sentences and labels.\n",
    "        \"\"\"\n",
    "        # Load data from files\n",
    "        positive_examples = list(open(\"./data/rt-polarity.pos\", \"r\", encoding='latin-1').readlines())\n",
    "        positive_examples = [s.strip() for s in positive_examples]\n",
    "        negative_examples = list(open(\"./data/rt-polarity.neg\", \"r\", encoding='latin-1').readlines())\n",
    "        negative_examples = [s.strip() for s in negative_examples]\n",
    "        #Split by words\n",
    "        x_text = [clean_str(sent) for sent in x_text]\n",
    "        x_text = [s.split(\" \") for s in x_text]\n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "        return [x_text, y]\n",
    "    \n",
    "    \n",
    "    def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "        \"\"\"\n",
    "        Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "        Returns padded sentences.\n",
    "        \"\"\"\n",
    "        sequence_length = max(len(x) for x in sentences)\n",
    "        padded_sentences = []\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_sentences.append(new_sentence)\n",
    "        return padded_sentences\n",
    "    \n",
    "    \n",
    "    def build_vocab(sentences):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary mapping from word to index based on the sentences.\n",
    "        Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        word_counts = Counter(itertools.chain(*sentences))\n",
    "        # Mapping from index to word\n",
    "        vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "        vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "        # Mapping from word to index\n",
    "        vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "        return [vocabulary, vocabulary_inv]\n",
    "    \n",
    "    \n",
    "    def build_input_data(sentences, labels, vocabulary):\n",
    "        \"\"\"\n",
    "        Maps sentences and labels to vectors based on a vocabulary.\n",
    "        \"\"\"\n",
    "        x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "        y = np.array(labels)\n",
    "        return [x, y]\n",
    "    \n",
    "    \n",
    "    def load_data():\n",
    "        \"\"\"\n",
    "        Loads and preprocessed data for the dataset.\n",
    "        Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "        \"\"\"\n",
    "        # Load and preprocess data\n",
    "        sentences, labels = load_data_and_labels()\n",
    "        sentences_padded = pad_sentences(sentences)\n",
    "        vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "        x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "        return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-c8f92ec2c6d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m  \u001b[1;31m#Split by words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreguntas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pregunta\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m  \u001b[1;31m# Generate labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-c8f92ec2c6d3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m  \u001b[1;31m#Split by words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreguntas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Pregunta\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m  \u001b[1;31m# Generate labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-dee9cfa0275a>\u001b[0m in \u001b[0;36mclean_str\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mOriginal\u001b[0m \u001b[0mtaken\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mgithub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0myoonkim\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mCNN_sentence\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \"\"\"\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"[^A-Za-z0-9(),!?\\'\\`]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\'s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \\'s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\'ve\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \\'ve\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Load data from files\n",
    "preguntas = pd.read_excel(\"data/Habitat_Habi_PreguntasRespuestas_v1.1-2.xlsx\")\n",
    " #Split by words\n",
    "x_text = preguntas[\"Pregunta\"].str.split().tolist()\n",
    "x_text = [clean_str(sent) for sent in x_text]\n",
    "x_text = [s.split(\" \") for s in x_text]\n",
    " # Generate labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(preguntas[\"Respuesta\"])\n",
    "y = le.transform(preguntas[\"Respuesta\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preguntas = pd.read_excel(\"data/Habitat_Habi_PreguntasRespuestas_v1.1-2.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas['Pregunta'] = preguntas['Pregunta'].map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load_data_and_labels():\n",
    "        \"\"\"\n",
    "        Loads polarity data from files, splits the data into words and generates labels.\n",
    "        Returns split sentences and labels.\n",
    "        \"\"\"\n",
    "        # Load data from files\n",
    "        positive_examples = list(open(\"./data/rt-polarity.pos\", \"r\", encoding='latin-1').readlines())\n",
    "        positive_examples = [s.strip() for s in positive_examples]\n",
    "        negative_examples = list(open(\"./data/rt-polarity.neg\", \"r\", encoding='latin-1').readlines())\n",
    "        negative_examples = [s.strip() for s in negative_examples]\n",
    "        x_text = positive_examples + negative_examples\n",
    "        #Split by words\n",
    "        x_text = [clean_str(sent) for sent in x_text]\n",
    "        x_text = [s.split(\" \") for s in x_text]\n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "        return [x_text, y]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
